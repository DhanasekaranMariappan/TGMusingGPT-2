{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNSV9pQhZcFRJvgzwbimT0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88da31c4ba41449ba571ad31ddc3e7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7675b69367d4f5b8983a96b3fec20eb",
              "IPY_MODEL_13d9235d245b4ee7808f52a3eb19095f",
              "IPY_MODEL_9808dd70c6734e9da14d1bd973bbaf6b"
            ],
            "layout": "IPY_MODEL_223c4cb96fdc439fac467c587a9dddfe"
          }
        },
        "c7675b69367d4f5b8983a96b3fec20eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71fe13fd654445399b51abc455159d46",
            "placeholder": "​",
            "style": "IPY_MODEL_7742643836e245438525becfb869feb8",
            "value": "Map: 100%"
          }
        },
        "13d9235d245b4ee7808f52a3eb19095f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e6baab54dd1401d8a4a87d61466f325",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81bc2d7511d649148672f709a22d345e",
            "value": 4
          }
        },
        "9808dd70c6734e9da14d1bd973bbaf6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec497fe12ec44ff69a7d2834755d985e",
            "placeholder": "​",
            "style": "IPY_MODEL_7cf715f9e50a437fafbacf6418bc4f8a",
            "value": " 4/4 [00:00&lt;00:00, 68.18 examples/s]"
          }
        },
        "223c4cb96fdc439fac467c587a9dddfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71fe13fd654445399b51abc455159d46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7742643836e245438525becfb869feb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e6baab54dd1401d8a4a87d61466f325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81bc2d7511d649148672f709a22d345e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec497fe12ec44ff69a7d2834755d985e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cf715f9e50a437fafbacf6418bc4f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9697a1c7d8ff4b4f842a293de82c341a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66ba293ca48f457ba18b063f99394388",
              "IPY_MODEL_60931d2b0a7646b883ea029d31e9c243",
              "IPY_MODEL_da1a18dd0eae4a26bef9c4de51631103"
            ],
            "layout": "IPY_MODEL_6993dd82d66d478cb5fe3bf82233b03d"
          }
        },
        "66ba293ca48f457ba18b063f99394388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb94225f6403428db7b00a56e3c78d0f",
            "placeholder": "​",
            "style": "IPY_MODEL_fd5933cdd7f34e948787471ebd7af452",
            "value": "Map: 100%"
          }
        },
        "60931d2b0a7646b883ea029d31e9c243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_536dcf16a30f45c5a71281ae548bf5d1",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10eb60d721774c88a69dfe1b3634f37d",
            "value": 4
          }
        },
        "da1a18dd0eae4a26bef9c4de51631103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ce2bc70f22541419dc1e7461cfcbd86",
            "placeholder": "​",
            "style": "IPY_MODEL_b2c03365090c450384dc7d31e0d73f89",
            "value": " 4/4 [00:00&lt;00:00, 80.56 examples/s]"
          }
        },
        "6993dd82d66d478cb5fe3bf82233b03d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb94225f6403428db7b00a56e3c78d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd5933cdd7f34e948787471ebd7af452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "536dcf16a30f45c5a71281ae548bf5d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10eb60d721774c88a69dfe1b3634f37d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ce2bc70f22541419dc1e7461cfcbd86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2c03365090c450384dc7d31e0d73f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhanasekaranMariappan/TGMusingGPT-2/blob/main/TextgenerationGPT_2model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load GPT-2 Model & Tokenizer"
      ],
      "metadata": {
        "id": "eKnoEz-sqv0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step involves installing necessary libraries, and then loading the pre-trained GPT-2 model and its corresponding tokenizer from the Hugging Face transformers library. A padding token is also added to the tokenizer."
      ],
      "metadata": {
        "id": "IlJf_z9SAmwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers huggingface_hub fsspec datasets\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from datasets import load_dataset, Dataset\n",
        "import time\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "# Add a padding token to the tokenizer. GPT-2 doesn't have one by default.\n",
        "# Using the eos_token is a common workaround.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"GPT-2 model and tokenizer loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j2CNaHsqIX0",
        "outputId": "6e5721c0-b9c1-4423-88f4-460d6e3e875a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "GPT-2 model and tokenizer loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading a dataset"
      ],
      "metadata": {
        "id": "rYmJBzMUq7yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step focuses on defining a custom dataset, which in this case is a simple list of strings."
      ],
      "metadata": {
        "id": "31vwWjumArfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try loading a different dataset to diagnose\n",
        "try:\n",
        "    test_dataset = load_dataset(\"emotion\", split=\"train\", streaming=True)\n",
        "    print(\"Successfully loaded a different dataset (emotion).\")\n",
        "    # You can iterate a few samples to confirm\n",
        "    # for i, example in enumerate(test_dataset):\n",
        "    #     if i == 5: break\n",
        "    #     print(example)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load the 'emotion' dataset: {e}\")\n",
        "\n",
        "# If the above was successful, try loading wikitext-2 again\n",
        "try:\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "    print(\"Successfully loaded wikitext-2-raw-v1 dataset.\")\n",
        "    print(dataset)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load the wikitext-2-raw-v1 dataset after updates: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qysDAS15rCES",
        "outputId": "6919c7a7-b9a9-464f-a9c2-5ab06db534fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded a different dataset (emotion).\n",
            "Successfully loaded wikitext-2-raw-v1 dataset.\n",
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 36718\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare custom dataset"
      ],
      "metadata": {
        "id": "ILFYBGWfrSrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This custom data is then converted into a format compatible with the Hugging Face datasets library."
      ],
      "metadata": {
        "id": "pu8LTG14BBHC"
      }
    },
    {
      "source": [
        "# Example: Using a simple list of strings as a dataset\n",
        "custom_texts = [\n",
        "    \"This is the first sentence of my custom dataset.\",\n",
        "    \"Here is another example sentence for fine-tuning GPT-2.\",\n",
        "    \"The third sentence continues the pattern of example data.\",\n",
        "    \"More text to help the model learn from custom data.\"\n",
        "]\n",
        "\n",
        "# To work with the `datasets` library, you might want to convert this\n",
        "# into a structure compatible with `Dataset.from_dict` or similar.\n",
        "# A simple way is to create a dictionary where each key is a column name\n",
        "# and the value is a list of data for that column.\n",
        "custom_data = {'text': custom_texts}\n",
        "\n",
        "# Create a Hugging Face Dataset from the custom data\n",
        "custom_dataset = Dataset.from_dict(custom_data)\n",
        "\n",
        "print(\"Custom dataset created successfully:\")\n",
        "print(custom_dataset)\n",
        "\n",
        "# You might need to tokenize this dataset before training.\n",
        "# The tokenization would depend on the specific task (e.g., text generation, classification).\n",
        "# For language modeling, you would typically concatenate the texts and then tokenize.\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # The tokenizer variable should be defined in the global scope before this function is called\n",
        "    # Ensure padding is set to max_length when truncating\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=tokenizer.model_max_length, padding=\"max_length\")\n",
        "\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_custom_dataset = custom_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"]) # Remove the original text column\n",
        "\n",
        "print(\"\\nTokenized custom dataset:\")\n",
        "print(tokenized_custom_dataset)\n",
        "\n",
        "# If you intend to use this for language modeling, you'll likely need to\n",
        "# prepare the data further, e.g., group and chunk texts.\n",
        "# Example (this is a common pattern for causal language modeling):\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    # Only concatenate tokenized columns (input_ids and attention_mask)\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys() if k in [\"input_ids\", \"attention_mask\"]}\n",
        "    total_length = len(concatenated_examples[list(concatenated_examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
        "    max_length = tokenizer.model_max_length # Typically 1024 for gpt2\n",
        "    # Adjust total_length to be a multiple of max_length for consistent chunking\n",
        "    total_length = (total_length // max_length) * max_length\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "lm_dataset = tokenized_custom_dataset.map(group_texts, batched=True)\n",
        "\n",
        "print(\"\\nLanguage model prepared custom dataset:\")\n",
        "print(lm_dataset)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390,
          "referenced_widgets": [
            "88da31c4ba41449ba571ad31ddc3e7ab",
            "c7675b69367d4f5b8983a96b3fec20eb",
            "13d9235d245b4ee7808f52a3eb19095f",
            "9808dd70c6734e9da14d1bd973bbaf6b",
            "223c4cb96fdc439fac467c587a9dddfe",
            "71fe13fd654445399b51abc455159d46",
            "7742643836e245438525becfb869feb8",
            "1e6baab54dd1401d8a4a87d61466f325",
            "81bc2d7511d649148672f709a22d345e",
            "ec497fe12ec44ff69a7d2834755d985e",
            "7cf715f9e50a437fafbacf6418bc4f8a",
            "9697a1c7d8ff4b4f842a293de82c341a",
            "66ba293ca48f457ba18b063f99394388",
            "60931d2b0a7646b883ea029d31e9c243",
            "da1a18dd0eae4a26bef9c4de51631103",
            "6993dd82d66d478cb5fe3bf82233b03d",
            "eb94225f6403428db7b00a56e3c78d0f",
            "fd5933cdd7f34e948787471ebd7af452",
            "536dcf16a30f45c5a71281ae548bf5d1",
            "10eb60d721774c88a69dfe1b3634f37d",
            "1ce2bc70f22541419dc1e7461cfcbd86",
            "b2c03365090c450384dc7d31e0d73f89"
          ]
        },
        "id": "wBaOXC1lyWvP",
        "outputId": "40128dfe-f2e0-4264-da1a-98399a51bef5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom dataset created successfully:\n",
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 4\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88da31c4ba41449ba571ad31ddc3e7ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized custom dataset:\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 4\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9697a1c7d8ff4b4f842a293de82c341a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Language model prepared custom dataset:\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 4\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Data for Language Modeling:\n",
        "The tokenized custom dataset is further processed for causal language modeling. This involves concatenating the tokenized texts and splitting them into fixed-size chunks, which is a standard practice for training language models.\n",
        "#Fine-tune the GPT-2 Model:\n",
        "This is the core training step. It involves defining training arguments, setting up a Trainer object from the transformers library with the fine-tuned model, training arguments, prepared dataset, and a data collator. The trainer.train() method is then called to perform the fine-tuning.\n",
        "#Save the Fine-tuned Model:\n",
        "After fine-tuning is complete, the trained model and its tokenizer are saved to a specified directory."
      ],
      "metadata": {
        "id": "m89GKVurBneS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, GPT2LMHeadModel, GPT2Tokenizer\n",
        "import time\n",
        "\n",
        "# Define the model name (if not defined elsewhere)\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Load the GPT-2 model and tokenizer (Ensure these lines are executed before DataCollator)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Add padding token\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_finetuned\",  # Output directory\n",
        "    overwrite_output_dir=True,  # Overwrite the content of the output directory\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=2,  # Batch size for training\n",
        "    save_steps=10_000,  # Save checkpoint every X updates steps\n",
        "    save_total_limit=2,  # Limit the total amount of checkpoints\n",
        "    logging_dir='./logs',  # Directory for storing logs\n",
        "    logging_steps=200,\n",
        "    eval_strategy=\"no\", # No evaluation during training\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    gradient_accumulation_steps=1,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "# Using DataCollatorForLanguageModeling for causal language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False # mlm=False for causal language modeling (GPT-2)\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "# Note: lm_dataset needs to be defined before this cell is run\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_dataset, # Use the prepared language model dataset\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "print(\"\\nStarting fine-tuning...\")\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "print(f\"Fine-tuning completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "trainer.save_model(\"./gpt2_finetuned_custom_data\")\n",
        "tokenizer.save_pretrained(\"./gpt2_finetuned_custom_data\")\n",
        "\n",
        "print(\"Fine-tuned model saved to ./gpt2_finetuned_custom_data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "SkyZCmg_0Kr0",
        "outputId": "c757fd96-cd90-4a8b-bc0a-a4eb18b52321"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhanasekaranmari2202\u001b[0m (\u001b[33mdhanasekaranmari2202-individual\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250612_112806-gowme75n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dhanasekaranmari2202-individual/huggingface/runs/gowme75n' target=\"_blank\">./gpt2_finetuned</a></strong> to <a href='https://wandb.ai/dhanasekaranmari2202-individual/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dhanasekaranmari2202-individual/huggingface' target=\"_blank\">https://wandb.ai/dhanasekaranmari2202-individual/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dhanasekaranmari2202-individual/huggingface/runs/gowme75n' target=\"_blank\">https://wandb.ai/dhanasekaranmari2202-individual/huggingface/runs/gowme75n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 04:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning completed in 858.92 seconds.\n",
            "Fine-tuned model saved to ./gpt2_finetuned_custom_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Fine-tuned Model and Generate Text:\n",
        "This step involves loading the saved fine-tuned model and tokenizer. A function is defined to take a prompt and use the loaded model to generate new text based on the prompt. Different generation strategies like greedy search and beam search are demonstrated."
      ],
      "metadata": {
        "id": "nfyrrCWPCyfE"
      }
    },
    {
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "fine_tuned_model_path = \"./gpt2_finetuned_custom_data\"\n",
        "model = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Ensure the padding token is set if not already saved in the tokenizer config\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Fine-tuned model and tokenizer loaded from {fine_tuned_model_path}\")\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=50, num_return_sequences=1, temperature=1.0, top_k=50, top_p=0.95):\n",
        "    # Encode the prompt and get attention mask\n",
        "    # We need to explicitly include the attention mask here\n",
        "    encoded_inputs = tokenizer.encode_plus(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True, # Ensure padding is applied if needed, though not strictly necessary for a single prompt\n",
        "        truncation=True, # Truncate if the prompt is too long\n",
        "        max_length=tokenizer.model_max_length\n",
        "    )\n",
        "    input_ids = encoded_inputs[\"input_ids\"]\n",
        "    attention_mask = encoded_inputs[\"attention_mask\"]\n",
        "\n",
        "\n",
        "    # Set num_beams to be at least num_return_sequences for beam search\n",
        "    # If num_return_sequences > 1, we need to enable beam search (num_beams > 1)\n",
        "    num_beams = max(num_return_sequences, 1)\n",
        "\n",
        "    # Generate text\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask, # Pass the attention mask\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        temperature=temperature, # Temperature is typically used with sampling, not beam search\n",
        "        # top_k=top_k, # top_k is typically used with sampling, not beam search\n",
        "        # top_p=top_p, # top_p is typically used with sampling, not beam search\n",
        "        num_beams=num_beams, # Enable beam search if num_return_sequences > 1\n",
        "        early_stopping=True, # Recommended for beam search\n",
        "        pad_token_id=tokenizer.eos_token_id, # Use eos_token_id for padding\n",
        "        no_repeat_ngram_size=2, # Optional: Prevent repeating n-grams\n",
        "    )\n",
        "\n",
        "    # Decode and print the generated text\n",
        "    print(\"\\nGenerated Text:\")\n",
        "    for i, generated_sequence in enumerate(output_sequences):\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        print(f\"Sequence {i+1}: {text.strip()}\")\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"This is a sentence to start the generation.\"\n",
        "generate_text(prompt, max_length=100, num_return_sequences=3) # This will now use beam search\n",
        "\n",
        "prompt = \"Here's another prompt:\"\n",
        "generate_text(prompt, max_length=80) # This will use greedy search (num_return_sequences=1, num_beams=1)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B4_KJXy8Q3W",
        "outputId": "4061a942-83a6-4d25-9198-0d5e63bca809"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model and tokenizer loaded from ./gpt2_finetuned_custom_data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "Sequence 1: This is a sentence to start the generation.\n",
            "\n",
            "Generate a list of all the items in the list. Each item is an array of integers. The first element is the name of the item, and the second element contains the number of items to be generated. For example, if you want to generate the following list, you would use this command: $ echo \"Hello world!\" $ list = array ( \"hello\", \"world\" ) $ print ( list ) # Generate the\n",
            "Sequence 2: This is a sentence to start the generation.\n",
            "\n",
            "Generate a list of all the items in the list. Each item is an array of integers. The first element is the name of the item, and the second element contains the number of items to be generated. For example, if you want to generate the following list, you would use this command: $ echo \"Hello world!\" $ list = array ( \"hello\", \"world\" ) $ generator = generator ( list ) # Gener\n",
            "Sequence 3: This is a sentence to start the generation.\n",
            "\n",
            "Generate a list of all the items in the list. Each item is an array of integers. The first element is the name of the item, and the second element contains the number of items to be generated. For example, if you want to generate the following list, you would use this command: $ echo \"Hello world!\" $ list = array ( \"hello\", \"world\" ) $ print ( list ) # Generate an\n",
            "\n",
            "Generated Text:\n",
            "Sequence 1: Here's another prompt:\n",
            "\n",
            "<input type=\"text\" name=\"input\" value=\"Enter your name\" />\n",
            ".\n",
            " (This is the same as the input type of the first prompt.)\n",
            ", and. (The input is a string.) The input can be any number of characters. The first character is optional. If you want to use the default value, you can use a\n"
          ]
        }
      ]
    }
  ]
}