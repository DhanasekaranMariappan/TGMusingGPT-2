{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNSV9pQhZcFRJvgzwbimT0F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load GPT-2 Model & Tokenizer"
      ],
      "metadata": {
        "id": "eKnoEz-sqv0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step involves installing necessary libraries, and then loading the pre-trained GPT-2 model and its corresponding tokenizer from the Hugging Face transformers library. A padding token is also added to the tokenizer."
      ],
      "metadata": {
        "id": "IlJf_z9SAmwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers huggingface_hub fsspec datasets\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from datasets import load_dataset, Dataset\n",
        "import time\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "# Add a padding token to the tokenizer. GPT-2 doesn't have one by default.\n",
        "# Using the eos_token is a common workaround.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"GPT-2 model and tokenizer loaded successfully.\")"
      ],
      "metadata": {
        "id": "8j2CNaHsqIX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading a dataset"
      ],
      "metadata": {
        "id": "rYmJBzMUq7yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step focuses on defining a custom dataset, which in this case is a simple list of strings."
      ],
      "metadata": {
        "id": "31vwWjumArfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try loading a different dataset to diagnose\n",
        "try:\n",
        "    test_dataset = load_dataset(\"emotion\", split=\"train\", streaming=True)\n",
        "    print(\"Successfully loaded a different dataset (emotion).\")\n",
        "    # You can iterate a few samples to confirm\n",
        "    # for i, example in enumerate(test_dataset):\n",
        "    #     if i == 5: break\n",
        "    #     print(example)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load the 'emotion' dataset: {e}\")\n",
        "\n",
        "# If the above was successful, try loading wikitext-2 again\n",
        "try:\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "    print(\"Successfully loaded wikitext-2-raw-v1 dataset.\")\n",
        "    print(dataset)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load the wikitext-2-raw-v1 dataset after updates: {e}\")"
      ],
      "metadata": {
        "id": "qysDAS15rCES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare custom dataset"
      ],
      "metadata": {
        "id": "ILFYBGWfrSrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This custom data is then converted into a format compatible with the Hugging Face datasets library."
      ],
      "metadata": {
        "id": "pu8LTG14BBHC"
      }
    },
    {
      "source": [
        "# Example: Using a simple list of strings as a dataset\n",
        "custom_texts = [\n",
        "    \"This is the first sentence of my custom dataset.\",\n",
        "    \"Here is another example sentence for fine-tuning GPT-2.\",\n",
        "    \"The third sentence continues the pattern of example data.\",\n",
        "    \"More text to help the model learn from custom data.\"\n",
        "]\n",
        "\n",
        "# To work with the `datasets` library, you might want to convert this\n",
        "# into a structure compatible with `Dataset.from_dict` or similar.\n",
        "# A simple way is to create a dictionary where each key is a column name\n",
        "# and the value is a list of data for that column.\n",
        "custom_data = {'text': custom_texts}\n",
        "\n",
        "# Create a Hugging Face Dataset from the custom data\n",
        "custom_dataset = Dataset.from_dict(custom_data)\n",
        "\n",
        "print(\"Custom dataset created successfully:\")\n",
        "print(custom_dataset)\n",
        "\n",
        "# You might need to tokenize this dataset before training.\n",
        "# The tokenization would depend on the specific task (e.g., text generation, classification).\n",
        "# For language modeling, you would typically concatenate the texts and then tokenize.\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # The tokenizer variable should be defined in the global scope before this function is called\n",
        "    # Ensure padding is set to max_length when truncating\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=tokenizer.model_max_length, padding=\"max_length\")\n",
        "\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_custom_dataset = custom_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"]) # Remove the original text column\n",
        "\n",
        "print(\"\\nTokenized custom dataset:\")\n",
        "print(tokenized_custom_dataset)\n",
        "\n",
        "# If you intend to use this for language modeling, you'll likely need to\n",
        "# prepare the data further, e.g., group and chunk texts.\n",
        "# Example (this is a common pattern for causal language modeling):\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    # Only concatenate tokenized columns (input_ids and attention_mask)\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys() if k in [\"input_ids\", \"attention_mask\"]}\n",
        "    total_length = len(concatenated_examples[list(concatenated_examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
        "    max_length = tokenizer.model_max_length # Typically 1024 for gpt2\n",
        "    # Adjust total_length to be a multiple of max_length for consistent chunking\n",
        "    total_length = (total_length // max_length) * max_length\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "lm_dataset = tokenized_custom_dataset.map(group_texts, batched=True)\n",
        "\n",
        "print(\"\\nLanguage model prepared custom dataset:\")\n",
        "print(lm_dataset)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wBaOXC1lyWvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Data for Language Modeling:\n",
        "The tokenized custom dataset is further processed for causal language modeling. This involves concatenating the tokenized texts and splitting them into fixed-size chunks, which is a standard practice for training language models.\n",
        "#Fine-tune the GPT-2 Model:\n",
        "This is the core training step. It involves defining training arguments, setting up a Trainer object from the transformers library with the fine-tuned model, training arguments, prepared dataset, and a data collator. The trainer.train() method is then called to perform the fine-tuning.\n",
        "#Save the Fine-tuned Model:\n",
        "After fine-tuning is complete, the trained model and its tokenizer are saved to a specified directory."
      ],
      "metadata": {
        "id": "m89GKVurBneS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, GPT2LMHeadModel, GPT2Tokenizer\n",
        "import time\n",
        "\n",
        "# Define the model name (if not defined elsewhere)\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Load the GPT-2 model and tokenizer (Ensure these lines are executed before DataCollator)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Add padding token\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_finetuned\",  # Output directory\n",
        "    overwrite_output_dir=True,  # Overwrite the content of the output directory\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=2,  # Batch size for training\n",
        "    save_steps=10_000,  # Save checkpoint every X updates steps\n",
        "    save_total_limit=2,  # Limit the total amount of checkpoints\n",
        "    logging_dir='./logs',  # Directory for storing logs\n",
        "    logging_steps=200,\n",
        "    eval_strategy=\"no\", # No evaluation during training\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    gradient_accumulation_steps=1,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "# Using DataCollatorForLanguageModeling for causal language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False # mlm=False for causal language modeling (GPT-2)\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "# Note: lm_dataset needs to be defined before this cell is run\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_dataset, # Use the prepared language model dataset\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "print(\"\\nStarting fine-tuning...\")\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "print(f\"Fine-tuning completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "trainer.save_model(\"./gpt2_finetuned_custom_data\")\n",
        "tokenizer.save_pretrained(\"./gpt2_finetuned_custom_data\")\n",
        "\n",
        "print(\"Fine-tuned model saved to ./gpt2_finetuned_custom_data\")"
      ],
      "metadata": {
        "id": "SkyZCmg_0Kr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Fine-tuned Model and Generate Text:\n",
        "This step involves loading the saved fine-tuned model and tokenizer. A function is defined to take a prompt and use the loaded model to generate new text based on the prompt. Different generation strategies like greedy search and beam search are demonstrated."
      ],
      "metadata": {
        "id": "nfyrrCWPCyfE"
      }
    },
    {
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "fine_tuned_model_path = \"./gpt2_finetuned_custom_data\"\n",
        "model = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Ensure the padding token is set if not already saved in the tokenizer config\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Fine-tuned model and tokenizer loaded from {fine_tuned_model_path}\")\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=50, num_return_sequences=1, temperature=1.0, top_k=50, top_p=0.95):\n",
        "    # Encode the prompt and get attention mask\n",
        "    # We need to explicitly include the attention mask here\n",
        "    encoded_inputs = tokenizer.encode_plus(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True, # Ensure padding is applied if needed, though not strictly necessary for a single prompt\n",
        "        truncation=True, # Truncate if the prompt is too long\n",
        "        max_length=tokenizer.model_max_length\n",
        "    )\n",
        "    input_ids = encoded_inputs[\"input_ids\"]\n",
        "    attention_mask = encoded_inputs[\"attention_mask\"]\n",
        "\n",
        "\n",
        "    # Set num_beams to be at least num_return_sequences for beam search\n",
        "    # If num_return_sequences > 1, we need to enable beam search (num_beams > 1)\n",
        "    num_beams = max(num_return_sequences, 1)\n",
        "\n",
        "    # Generate text\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask, # Pass the attention mask\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        temperature=temperature, # Temperature is typically used with sampling, not beam search\n",
        "        # top_k=top_k, # top_k is typically used with sampling, not beam search\n",
        "        # top_p=top_p, # top_p is typically used with sampling, not beam search\n",
        "        num_beams=num_beams, # Enable beam search if num_return_sequences > 1\n",
        "        early_stopping=True, # Recommended for beam search\n",
        "        pad_token_id=tokenizer.eos_token_id, # Use eos_token_id for padding\n",
        "        no_repeat_ngram_size=2, # Optional: Prevent repeating n-grams\n",
        "    )\n",
        "\n",
        "    # Decode and print the generated text\n",
        "    print(\"\\nGenerated Text:\")\n",
        "    for i, generated_sequence in enumerate(output_sequences):\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        print(f\"Sequence {i+1}: {text.strip()}\")\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"This is a sentence to start the generation.\"\n",
        "generate_text(prompt, max_length=100, num_return_sequences=3) # This will now use beam search\n",
        "\n",
        "prompt = \"Here's another prompt:\"\n",
        "generate_text(prompt, max_length=80) # This will use greedy search (num_return_sequences=1, num_beams=1)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4B4_KJXy8Q3W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}